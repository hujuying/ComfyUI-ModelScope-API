import requests
import json
import time
import torch
import numpy as np
from PIL import Image
from io import BytesIO
import os
import base64
import re
from .modelscope_image_node import load_config, save_config, tensor_to_base64_url

# æ£€æŸ¥openaiåº“æ˜¯å¦å¯ç”¨
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# ä»…ä¸modelscope_config.jsonäº¤äº’çš„API Tokenç®¡ç†å‡½æ•°
def load_api_tokens():
    try:
        cfg = load_config()
        tokens_from_cfg = cfg.get("api_tokens", [])
        if tokens_from_cfg and isinstance(tokens_from_cfg, list):
            return [token.strip() for token in tokens_from_cfg if token.strip()]
    except Exception as e:
        print(f"è¯»å–configä¸­çš„tokenså¤±è´¥: {e}")
    return []

def save_api_tokens(tokens):
    try:
        cfg = load_config()
        cfg["api_tokens"] = tokens
        return save_config(cfg)
    except Exception as e:
        print(f"ä¿å­˜tokensåˆ°configå¤±è´¥: {e}")
        return False

class ModelScopeImageCaptionNode:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        if not OPENAI_AVAILABLE:
            return {
                "required": {
                    "error_message": ("STRING", {
                        "default": "è¯·å…ˆå®‰è£…openaiåº“: pip install openai",
                        "multiline": True
                    }),
                }
            }
        saved_tokens = load_api_tokens()
        # å®šä¹‰æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        supported_models = [
            "Qwen/Qwen3-VL-8B-Instruct",
            "Qwen/Qwen3-VL-235B-A22B-Instruct"
        ]
        return {
            "required": {
                "api_tokens": ("STRING", {
                    "default": f"***å·²ä¿å­˜{len(saved_tokens)}ä¸ªToken***" if saved_tokens else "",
                    "placeholder": "è¯·è¾“å…¥API Tokenï¼ˆæ”¯æŒå¤šä¸ªï¼Œç”¨é€—å·/æ¢è¡Œåˆ†éš”ï¼‰",
                    "multiline": True
                }),
            },
            "optional": {
                # å…³é”®ä¿®æ”¹ï¼šå°†imageè®¾ç½®ä¸ºå¯é€‰è¾“å…¥
                "image": ("IMAGE", {"optional": True}),
                "prompt1": ("STRING", {
                    "multiline": True,
                    "default": "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»ä½“ã€èƒŒæ™¯ã€é¢œè‰²ã€é£æ ¼ç­‰ä¿¡æ¯"
                }),
                "prompt2": ("STRING", {
                    "multiline": True,
                    "default": ""
                }),
                "model": (supported_models, {
                    "default": "Qwen/Qwen3-VL-8B-Instruct"
                }),
                "max_tokens": ("INT", {
                    "default": 1000,
                    "min": 100,
                    "max": 4000
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("description",)
    FUNCTION = "generate_caption"
    CATEGORY = "ModelScopeAPI"

    def parse_api_tokens(self, token_input):
        """è§£æè¾“å…¥çš„å¤šä¸ªAPI Tokenï¼ˆæ”¯æŒé€—å·ã€åˆ†å·ã€æ¢è¡Œåˆ†éš”ï¼‰"""
        if not token_input or token_input.strip() in ["", f"***å·²ä¿å­˜{len(load_api_tokens())}ä¸ªToken***"]:
            return load_api_tokens()
        
        # æ”¯æŒå¤šç§åˆ†éš”ç¬¦æ‹†åˆ†Token
        tokens = re.split(r'[,;\n]+', token_input)
        return [token.strip() for token in tokens if token.strip()]
    
    def create_blank_image(self, width=64, height=64):
        """åˆ›å»ºç©ºç™½å›¾åƒå¼ é‡ï¼ˆç¬¦åˆComfyUIçš„å›¾åƒæ ¼å¼è¦æ±‚ï¼‰"""
        # åˆ›å»ºç™½è‰²èƒŒæ™¯çš„RGBå›¾åƒ
        blank_np = np.ones((height, width, 3), dtype=np.uint8) * 255
        # è½¬æ¢ä¸ºComfyUIæ ¼å¼çš„å¼ é‡ (batch, height, width, channels)
        blank_tensor = torch.from_numpy(blank_np).unsqueeze(0).float() / 255.0
        return blank_tensor

    def generate_caption(self, image=None, api_tokens="", prompt1="è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹", prompt2="", model="Qwen/Qwen3-VL-8B-Instruct", max_tokens=1000, temperature=0.7):
        if not OPENAI_AVAILABLE:
            return ("è¯·å…ˆå®‰è£…openaiåº“: pip install openai",)
        
        # å…³é”®ä¿®æ”¹ï¼šå¤„ç†è¾“å…¥å›¾åƒä¸ºç©ºçš„æƒ…å†µ
        if image is None:
            print("âš ï¸ æœªè¾“å…¥å›¾åƒï¼Œè‡ªåŠ¨ç”Ÿæˆç©ºç™½å›¾åƒä½œä¸ºè¾“å…¥")
            image = self.create_blank_image()
        
        # å¤„ç†æç¤ºè¯åˆå¹¶
        prompt_parts = []
        if prompt1.strip():
            prompt_parts.append(prompt1.strip())
        if prompt2.strip():
            prompt_parts.append(prompt2.strip())
        
        if not prompt_parts:
            prompt = "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»ä½“ã€èƒŒæ™¯ã€é¢œè‰²ã€é£æ ¼ç­‰ä¿¡æ¯"
        else:
            prompt = ", ".join(prompt_parts)
        
        # è§£æTokenåˆ—è¡¨
        tokens = self.parse_api_tokens(api_tokens)
        if not tokens:
            raise Exception("è¯·æä¾›è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆçš„API Token")
        
        # ä¿å­˜æ–°Tokenï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
        saved_tokens = load_api_tokens()
        if api_tokens.strip() not in ["", f"***å·²ä¿å­˜{len(saved_tokens)}ä¸ªToken***"]:
            if save_api_tokens(tokens):
                print(f"âœ… å·²ä¿å­˜ {len(tokens)} ä¸ªAPI Token")
            else:
                print("âš ï¸ API Tokenä¿å­˜å¤±è´¥ï¼Œä½†ä¸å½±å“å½“å‰ä½¿ç”¨")
        
        try:
            print(f"ğŸ” å¼€å§‹ç”Ÿæˆå›¾åƒæè¿°...")
            print(f"ğŸ“ æç¤ºè¯: {prompt}")
            print(f"ğŸ¤– æ¨¡å‹: {model}")
            print(f"ğŸ”‘ å¯ç”¨Tokenæ•°é‡: {len(tokens)}")
            
            # è½¬æ¢å›¾åƒä¸ºbase64æ ¼å¼
            image_url = tensor_to_base64_url(image)
            print(f"ğŸ–¼ï¸ å›¾åƒå·²è½¬æ¢ä¸ºbase64æ ¼å¼")
            
            # æ„å»ºæ¶ˆæ¯ä½“
            messages = [{
                'role': 'user',
                'content': [{
                    'type': 'text',
                    'text': prompt,
                }, {
                    'type': 'image_url',
                    'image_url': {
                        'url': image_url,
                    },
                }],
            }]
            
            # è½®è¯¢å°è¯•æ¯ä¸ªToken
            last_exception = None
            for i, token in enumerate(tokens):
                try:
                    print(f"ğŸ”„ å°è¯•ä½¿ç”¨ç¬¬ {i+1}/{len(tokens)} ä¸ªToken...")
                    
                    client = OpenAI(
                        base_url='https://api-inference.modelscope.cn/v1',
                        api_key=token
                    )
                    
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        stream=False
                    )
                    
                    description = response.choices[0].message.content
                    print(f"âœ… ç¬¬ {i+1} ä¸ªTokenè°ƒç”¨æˆåŠŸ!")
                    print(f"ğŸ“„ ç»“æœé¢„è§ˆ: {description[:100]}...")
                    return (description,)
                    
                except Exception as e:
                    last_exception = e
                    print(f"âŒ ç¬¬ {i+1} ä¸ªTokenè°ƒç”¨å¤±è´¥: {str(e)}")
                    if i < len(tokens) - 1:
                        print(f"â³ å‡†å¤‡å°è¯•ä¸‹ä¸€ä¸ªToken...")
            
            # æ‰€æœ‰Tokenéƒ½å¤±è´¥
            raise Exception(f"æ‰€æœ‰Tokenå‡è°ƒç”¨å¤±è´¥: {str(last_exception)}")
            
        except Exception as e:
            error_msg = f"å›¾åƒæè¿°ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return (error_msg,)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "ModelScopeImageCaptionNode": ModelScopeImageCaptionNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ModelScopeImageCaptionNode": "ModelScope å›¾åƒæè¿°ç”Ÿæˆ"
}